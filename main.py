#!/usr/bin/env python3
"""
Academic Performance Prediction using Machine Learning Classification
Enhanced with Interactive Visualizations and Advanced Metrics
"""

from src.data_loader import DataLoader
from src.models import ModelTrainer
from src.visualizer import Visualizer
from src.enhanced_visualizer import EnhancedVisualizer
from src.cross_validation import CrossValidator
from src.hyperparameter_tuning import HyperparameterTuner
from src.evaluation_metrics import ModelEvaluator
from src.data_validation import DataValidator
from src.config_manager import ConfigManager
from src.logger import MLLogger
from src.cli_parser import CLIParser
from src.report_generator import ReportGenerator
from src.benchmarking import PerformanceBenchmark
import pandas as pd
import time
import sys

def main():
    start_time = time.time()
    
    # Parse command line arguments
    cli_parser = CLIParser()
    args = cli_parser.parse_args()
    cli_parser.validate_args()
    
    print("üéì Academic Performance Prediction Project - Enhanced Edition")
    print("=" * 60)
    
    # Show CLI arguments if any were provided
    if len(sys.argv) > 1:
        cli_parser.print_args_summary()
    
    # Handle special modes
    if args.dry_run:
        print("\nüîç DRY RUN MODE - Showing configuration without execution")
    
    if args.run_tests:
        print("\nüß™ Running unit tests...")
        import subprocess
        result = subprocess.run([sys.executable, 'tests/run_tests.py'], 
                              capture_output=True, text=True)
        print(result.stdout)
        if result.returncode != 0:
            print("‚ùå Tests failed. Exiting.")
            sys.exit(1)
        print("‚úÖ All tests passed!")
    
    # Load configuration
    config = ConfigManager(args.config)
    
    # Update config with CLI arguments
    cli_parser.update_config_from_args(config)
    
    config.validate_config()
    config.create_directories()
    
    # Initialize logger
    logger = MLLogger(config)
    session_id = logger.create_session_log()
    
    # Save config if requested
    if args.save_config:
        config.save_config(args.save_config)
        print(f"‚úÖ Configuration saved to {args.save_config}")
    
    if not args.quiet:
        config.print_config()
    
    if args.dry_run:
        print("\n‚úÖ Dry run completed. Configuration validated.")
        return
    
    # Initialize components with configuration
    data_loader = DataLoader()
    model_trainer = ModelTrainer()
    visualizer = Visualizer(results_dir=config.get('visualization.results_dir'))
    enhanced_viz = EnhancedVisualizer(results_dir=config.get('visualization.results_dir'))
    cv_validator = CrossValidator(
        cv_folds=config.get('cross_validation.cv_folds'),
        random_state=config.get('cross_validation.random_state')
    )
    hp_tuner = HyperparameterTuner(
        cv_folds=config.get('hyperparameter_tuning.cv_folds'),
        n_jobs=config.get('hyperparameter_tuning.n_jobs'),
        random_state=config.get('hyperparameter_tuning.random_state')
    )
    evaluator = ModelEvaluator()
    validator = DataValidator()
    report_generator = ReportGenerator(config)
    
    # Initialize benchmarking if profiling is enabled
    benchmark = None
    if args.profile:
        benchmark = PerformanceBenchmark()
        logger.info("Performance profiling enabled")
    
    # Load and preprocess data
    if not args.quiet:
        print("\nüìä Loading and preprocessing data...")
    logger.info("Starting data loading and preprocessing")
    
    if args.data_file:
        # Load custom dataset
        try:
            df = pd.read_csv(args.data_file)
            logger.info(f"Loaded custom dataset from {args.data_file}")
        except Exception as e:
            logger.error(f"Failed to load custom dataset: {e}")
            sys.exit(1)
    else:
        # Generate sample data
        df = data_loader.generate_sample_data(n_samples=config.get('data.sample_size'))
    
    logger.log_data_info(df)
    
    if not args.quiet:
        print(f"Dataset shape: {df.shape}")
        print(f"Features: {list(df.columns)}")
    
    # Validate data quality
    if cli_parser.should_run_component('data_validation'):
        logger.info("Starting data validation")
        validator.validate_dataset(df, target_column=config.get('data.target_column'))
        if not args.quiet:
            validator.print_validation_report()
        quality_score = validator.get_data_quality_score()
        logger.log_data_validation(quality_score)
        if not args.quiet:
            print(f"\nüèÜ Data Quality Score: {quality_score:.1f}/100")
    
    # Show data info
    print("\nüìà Dataset Overview:")
    print(df.head())
    print(f"\nTarget distribution:")
    print(df['performance'].value_counts())
    
    # Enhanced visualizations
    if cli_parser.should_run_component('visualization'):
        if not args.quiet:
            print("\nüé® Creating enhanced interactive visualizations...")
        
        # Interactive data distribution
        if not args.quiet:
            print("  ‚Üí Interactive data distribution plots...")
        enhanced_viz.plot_interactive_data_distribution(df)
        
        # Correlation analysis
        if not args.quiet:
            print("  ‚Üí Feature correlation heatmap...")
        enhanced_viz.plot_correlation_heatmap(df)
    
    # Preprocess data
    X, y, feature_names = data_loader.preprocess_data(df)
    X_train, X_test, y_train, y_test = data_loader.split_data(
        X, y, 
        test_size=config.get('data.test_size'),
        random_state=config.get('data.random_state')
    )
    
    print(f"\nTraining set size: {X_train.shape[0]}")
    print(f"Test set size: {X_test.shape[0]}")
    
    # Train models
    print("\nü§ñ Training machine learning models...")
    logger.info("Starting model training")
    training_start = time.time()
    
    # Benchmark model training if profiling enabled
    if benchmark:
        training_benchmarks = benchmark.benchmark_model_training(
            model_trainer.models, X_train, y_train
        )
        # Use the last trained models
        model_trainer.trained_models = model_trainer.models
    else:
        model_trainer.train_models(X_train, y_train)
    
    training_end = time.time()
    logger.log_execution_time("Model training", training_start, training_end)
    
    # Perform cross-validation
    if cli_parser.should_run_component('cross_validation'):
        logger.info("Starting cross-validation")
        cv_validator.perform_cross_validation(model_trainer.models, X, y)
        if not args.quiet:
            cv_validator.print_cv_results()
        
        # Get CV summary
        cv_summary = cv_validator.get_cv_summary()
        if not args.quiet:
            print(f"\nüìã Cross-Validation Summary:")
            print(cv_summary.round(4))
    
    # Hyperparameter tuning
    if cli_parser.should_run_component('hyperparameter_tuning'):
        logger.info("Starting hyperparameter tuning")
        tuning_start = time.time()
        
        hp_tuner.tune_hyperparameters(
            X_train, y_train, 
            method=config.get('hyperparameter_tuning.method'),
            n_iter=config.get('hyperparameter_tuning.n_iter')
        )
        if not args.quiet:
            hp_tuner.print_tuning_results()
        
        tuning_end = time.time()
        logger.log_execution_time("Hyperparameter tuning", tuning_start, tuning_end)
        
        # Get tuning summary
        tuning_summary = hp_tuner.get_tuning_summary()
        if not args.quiet:
            print(f"\nüéØ Hyperparameter Tuning Summary:")
            print(tuning_summary)
    
    # Evaluate models
    print("\nüìä Evaluating models...")
    model_trainer.evaluate_models(X_test, y_test, data_loader.target_encoder)
    
    # Comprehensive evaluation with detailed metrics
    detailed_metrics = evaluator.evaluate_all_models(
        model_trainer.trained_models, X_test, y_test, data_loader.target_encoder
    )
    evaluator.print_detailed_results()
    
    # Get metrics comparison
    metrics_comparison = evaluator.get_metrics_comparison()
    print(f"\nüìã Comprehensive Metrics Comparison:")
    print(metrics_comparison)
    
    # Enhanced result visualizations
    print("\nüìà Creating enhanced result visualizations...")
    
    # Interactive model comparison
    print("  ‚Üí Interactive model comparison...")
    enhanced_viz.plot_interactive_model_comparison(model_trainer.results)
    
    # ROC Curves
    print("  ‚Üí ROC curves analysis...")
    enhanced_viz.plot_roc_curves(model_trainer.trained_models, X_test, y_test, data_loader.target_encoder)
    
    # Precision-Recall Curves
    print("  ‚Üí Precision-Recall curves...")
    enhanced_viz.plot_precision_recall_curves(model_trainer.trained_models, X_test, y_test, data_loader.target_encoder)
    
    # Enhanced confusion matrices
    print("  ‚Üí Enhanced confusion matrices...")
    enhanced_viz.plot_enhanced_confusion_matrices(model_trainer.results, data_loader.target_encoder)
    
    # Feature importance comparison
    print("  ‚Üí Feature importance comparison...")
    enhanced_viz.plot_feature_importance_comparison(model_trainer.trained_models, feature_names)
    
    # Performance dashboard
    print("  ‚Üí Performance dashboard...")
    enhanced_viz.create_model_performance_dashboard(model_trainer.results)
    
    # Original visualizations (for comparison)
    print("\nüìä Creating traditional visualizations...")
    visualizer.plot_data_distribution(df)
    visualizer.plot_model_comparison(model_trainer.results)
    visualizer.plot_confusion_matrices(model_trainer.results, data_loader.target_encoder)
    
    # Performance benchmarking
    if benchmark:
        logger.info("Running performance benchmarks")
        
        # Benchmark prediction speed
        prediction_benchmarks = benchmark.benchmark_prediction_speed(
            model_trainer.trained_models, X_test
        )
        
        # Compare model efficiency
        if 'training_benchmarks' in locals():
            efficiency_comparison = benchmark.compare_model_efficiency(
                training_benchmarks, prediction_benchmarks
            )
            
            if not args.quiet:
                print("\n‚ö° Performance Benchmarks:")
                print("-" * 40)
                for model_name, metrics in efficiency_comparison.items():
                    print(f"{model_name}:")
                    print(f"  Training Efficiency: {metrics['training_efficiency']:.4f}")
                    print(f"  Prediction Speed: {metrics['prediction_speed']:.2f} pred/sec")
                    print(f"  Overall Efficiency: {metrics['overall_efficiency']:.4f}")
        
        # Save benchmark report
        benchmark_files = benchmark.save_benchmark_report(
            output_dir=config.get('visualization.results_dir', 'results')
        )
        
        if not args.quiet:
            print(f"\nüìä Benchmark reports saved:")
            for file_path in benchmark_files:
                print(f"   ‚Ä¢ {file_path}")
        
        benchmark.print_benchmark_summary()
    # Get best model and show feature importance
    best_name, best_model = model_trainer.get_best_model()
    logger.log_best_model(best_name, model_trainer.results[best_name]['accuracy'])
    
    print(f"\nüèÜ Best performing model: {best_name}")
    print(f"Best accuracy: {model_trainer.results[best_name]['accuracy']:.4f}")
    
    # Plot feature importance for best model
    visualizer.plot_feature_importance(best_model, feature_names, best_name)
    
    # Save best model
    model_path = f'models/best_model_{best_name.replace(" ", "_")}.pkl'
    model_trainer.save_model(best_name, best_model, model_path)
    logger.info(f"Best model saved to {model_path}")
    
    # End session and generate report
    end_time = time.time()
    execution_times = {
        'total': end_time - start_time,
        'training': training_end - training_start if 'training_end' in locals() else 0,
        'tuning': tuning_end - tuning_start if 'tuning_end' in locals() else 0
    }
    
    logger.log_execution_time("Total execution", start_time, end_time)
    logger.end_session_log(session_id)
    
    # Generate comprehensive report
    if config.get('output.generate_report', True):
        logger.info("Generating comprehensive report")
        
        data_info = {
            'shape': df.shape,
            'features': list(df.columns),
            'target_distribution': df['performance'].value_counts().to_dict()
        }
        
        best_model_info = {
            'name': best_name,
            'accuracy': model_trainer.results[best_name]['accuracy']
        }
        
        report_generator.generate_comprehensive_report(
            data_info=data_info,
            validation_results={'quality_score': quality_score} if 'quality_score' in locals() else None,
            cv_results=cv_validator.cv_results if cli_parser.should_run_component('cross_validation') else None,
            tuning_results=hp_tuner.tuning_results if cli_parser.should_run_component('hyperparameter_tuning') else None,
            evaluation_results=evaluator.detailed_results,
            best_model_info=best_model_info,
            execution_times=execution_times
        )
        
        # Save report in multiple formats
        report_files = report_generator.save_report(
            output_dir=config.get('visualization.results_dir', 'results'),
            formats=config.get('output.results_format', ['json', 'html'])
        )
        
        if not args.quiet:
            print(f"\nüìã Comprehensive report generated:")
            for file_path in report_files:
                print(f"   ‚Ä¢ {file_path}")
    
    if not args.quiet:
        print("\n‚úÖ Enhanced analysis complete!")
        print("üìÅ Check the 'results' folder for:")
        print("   ‚Ä¢ Interactive HTML visualizations")
        print("   ‚Ä¢ High-resolution PNG images")
        print("   ‚Ä¢ ROC and Precision-Recall curves")
        print("   ‚Ä¢ Performance dashboard")
        print("   ‚Ä¢ Feature importance analysis")
        print("   ‚Ä¢ Comprehensive ML report")
        print("üìÅ Best model saved in 'models' folder.")
        print("üìã Detailed logs saved in 'logs' folder.")

if __name__ == "__main__":
    main()